{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "im in trable that my github page always cant understand new version of ten. and keras. so i dont know how to do.\n",
        "my ai said thats because new variable is set in new ten. and keras, but github page dont understand new. can you explain these in chinese?\n",
        "è½èµ·ä¾†ä½ é‡åˆ°çš„å•é¡Œä¸»è¦æ˜¯ ç‰ˆæœ¬ç›¸å®¹æ€§ ç”¢ç”Ÿçš„æ–·å±¤ã€‚ç°¡å–®ä¾†èªªï¼Œä½ åœ¨é›»è…¦ä¸Šç”¨ Python è¨“ç·´çš„æ¨¡å‹å¤ªã€Œæ–°ã€äº†ï¼Œè€Œç€è¦½å™¨ç«¯çš„ TensorFlow.js (TF.js) æˆ–æ˜¯ GitHub Pages è¼‰å…¥çš„ç’°å¢ƒè·Ÿä¸ä¸Šã€‚\n",
        "é€™è£¡è¦å…ˆç³¾æ­£ä¸€å€‹è§€å¿µï¼šGitHub Pages æœ¬èº«ä¸¦ä¸ã€Œç†è§£ã€ç¨‹å¼ç¢¼ï¼Œå®ƒåªæ˜¯ä¸€å€‹åƒã€Œæª”æ¡ˆæ«ƒã€çš„ç©ºé–“ã€‚å•é¡Œé€šå¸¸å‡ºåœ¨ä½ çš„ JavaScript ç¨‹å¼ç¢¼ç‰ˆæœ¬ æˆ– è½‰æ›å·¥å…·ç‰ˆæœ¬ ä¸ä¸€è‡´ã€‚\n",
        "ä»¥ä¸‹æ˜¯ç‚ºä»€éº¼æœƒç™¼ç”ŸéŒ¯èª¤ï¼Œä»¥åŠè©²å¦‚ä½•è§£æ±ºï¼š\n",
        "1. ç‚ºä»€éº¼æœƒå ±éŒ¯ï¼Ÿ (æ–°èˆŠç‰ˆæœ¬çš„è¡çª)\n",
        "ç•¶ä½ åœ¨ Python ç’°å¢ƒä½¿ç”¨æœ€æ–°çš„ TensorFlow 2.x æˆ– Keras æ™‚ï¼Œæ¨¡å‹æœƒåŒ…å«ä¸€äº›æ–°çš„é‹ç®—å…ƒï¼ˆOperationsï¼‰æˆ–åƒæ•¸çµæ§‹ã€‚\n",
        "è½‰æ›å™¨ç‰ˆæœ¬å¤ªèˆŠï¼š å¦‚æœä½ ç”¨èˆŠç‰ˆçš„ tensorflowjs_converter å»è½‰æ–°æ¨¡å‹ï¼Œè¼¸å‡ºçš„ model.json å¯èƒ½æ ¼å¼éŒ¯èª¤ã€‚\n",
        "å‰ç«¯ç¶²é åº«å¤ªèˆŠï¼š å¦‚æœä½ åœ¨ HTML è£¡å¼•å…¥çš„æ˜¯èˆŠç‰ˆçš„ TF.jsï¼ˆä¾‹å¦‚ v1.x æˆ– v2.xï¼‰ï¼Œå®ƒå°±çœ‹ä¸æ‡‚æ–°ç‰ˆ Keras ç”¢ç”Ÿçš„æ–°è®Šæ•¸ã€‚\n",
        "2. æ ¸å¿ƒè§£æ±ºæ–¹æ¡ˆï¼šåŒæ­¥ç‰ˆæœ¬\n",
        "è¦è§£æ±ºé€™å€‹å•é¡Œï¼Œè«‹æŒ‰ç…§ä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿæª¢æŸ¥ï¼š\n",
        "ç¬¬ä¸€æ­¥ï¼šç¢ºä¿ Python è½‰æ›å·¥å…·æ˜¯æœ€æ–°çš„\n",
        "åœ¨ä½ çš„é–‹ç™¼ç’°å¢ƒåŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼Œæ›´æ–°è½‰æ›å™¨ï¼š\n",
        "bash\n",
        "pip install --upgrade tensorflowjs\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "ç¬¬äºŒæ­¥ï¼šé‡æ–°è½‰æ›æ¨¡å‹\n",
        "ä½¿ç”¨æ›´æ–°å¾Œçš„å·¥å…·é‡æ–°ç”¢å‡º model.json å’Œæ¬Šé‡æª”ï¼ˆ.binï¼‰ã€‚\n",
        "ç¬¬ä¸‰æ­¥ï¼šåœ¨ GitHub Pages çš„ HTML è£¡å¼•å…¥æœ€æ–°ç‰ˆ TF.js\n",
        "é€™é€šå¸¸æ˜¯å‡ºéŒ¯çš„åœ°æ–¹ã€‚è«‹æª¢æŸ¥ä½ çš„ index.htmlï¼Œç¢ºä¿ <script> æ¨™ç±¤æŒ‡å‘çš„æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼ˆç›®å‰æ¨è–¦ 4.x ç‰ˆæœ¬ï¼‰ï¼š\n",
        "html\n",
        "<!-- ä¸è¦ä½¿ç”¨å›ºå®šçš„å°ç‰ˆæœ¬è™Ÿï¼Œå»ºè­°ä½¿ç”¨æœ€æ–°ç‰ˆ -->\n",
        "<script src=\"https://cdn.jsdelivr.net\"></script>\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "3. å¸¸è¦‹çš„å‘ï¼šè·¯å¾‘èˆ‡å¿«å–\n",
        "å¦‚æœç‰ˆæœ¬éƒ½å°äº†é‚„æ˜¯ä¸è¡Œï¼Œè«‹æª¢æŸ¥ï¼š\n",
        "å¤§å°å¯«æ•æ„Ÿï¼š åœ¨é›»è…¦ä¸Š Model.json å’Œ model.json å¯èƒ½æ²’å·®åˆ¥ï¼Œä½†åœ¨ GitHub Pagesï¼ˆLinux ä¼ºæœå™¨ï¼‰ä¸Šæ˜¯å®Œå…¨ä¸åŒçš„æª”æ¡ˆã€‚\n",
        "ç€è¦½å™¨å¿«å–ï¼š å¦³æ›´æ–°äº†æª”æ¡ˆåˆ° GitHubï¼Œä½†ç€è¦½å™¨é‚„åœ¨è®€èˆŠçš„ç‰ˆæœ¬ã€‚è«‹å˜—è©¦ç”¨ ç„¡ç—•æ¨¡å¼ (Incognito) é–‹å•Ÿç¶²é ã€‚\n",
        "4. æª¢æŸ¥éŒ¯èª¤è¨Šæ¯\n",
        "å¦‚æœç¶²é é‚„æ˜¯è·‘ä¸å‹•ï¼Œè«‹åœ¨ GitHub Pages é é¢æŒ‰ä¸‹ F12 æ‰“é–‹ é–‹ç™¼è€…å·¥å…· (Console)ã€‚\n",
        "å¦‚æœçœ‹åˆ° Unknown Layer æˆ– Unknown Opï¼šè¡¨ç¤ºä½ çš„ Keras æ¨¡å‹è£¡ç”¨äº† TF.js é‚„ä¸æ”¯æ´çš„åŠŸèƒ½ã€‚\n",
        "å¦‚æœçœ‹åˆ° 404 Not Foundï¼šè¡¨ç¤ºæ¨¡å‹æª”æ¡ˆçš„è·¯å¾‘å¯«éŒ¯äº†ã€‚\n",
        "ä½ æƒ³è®“æˆ‘å¹«ä½ æª¢æŸ¥ä¸€ä¸‹ä½ çš„ HTML å¼•å…¥ç¢¼ï¼ˆCDN é€£çµï¼‰ï¼Œæˆ–æ˜¯å¹«ä½ å¯«ä¸€æ®µæ¸¬è©¦æ¨¡å‹æ˜¯å¦è¼‰å…¥æˆåŠŸçš„ JavaScript ç¨‹å¼ç¢¼å—ï¼Ÿ\n",
        "\n",
        "\n",
        "\n",
        "all web code :https://github.com/Penter405/recycle/tree/main/docs\n",
        "According to an analysis of the provided GitHub repository code, key issues preventing the model from running include an outdated TensorFlow.js version and syntax errors []. The response suggests updating the TensorFlow.js version in index.html to the latest version (4.22.0) and modifying script.js to correctly load the model and handle predictions []. It also emphasizes ensuring the model files (model.json and associated .bin files) are in the correct directory structure []. The analysis notes that \"new variable\" errors might stem from using non-standard layers or custom functions in the Keras model, suggesting a check for layers like Normalization or Rescaling if errors persist after updating TF.js []. More information can be found at the Penter405 GitHub repository.\n",
        "\n",
        "\n",
        "\n",
        "chinese\n",
        "æˆ‘æŸ¥çœ‹äº†ä½ çš„ GitHub ä»£ç¢¼ (recycle/docs)ï¼Œç™¼ç¾äº†å¹¾å€‹å°è‡´ã€Œæ–°ç‰ˆæœ¬ Keras è®Šæ•¸ç„¡æ³•è­˜åˆ¥ã€çš„æ ¸å¿ƒå•é¡Œã€‚\n",
        "ä»¥ä¸‹æ˜¯é‡å°ä½ ç¨‹å¼ç¢¼çš„ä¿®æ­£å»ºè­°ï¼š\n",
        "1. ä¿®æ”¹ index.htmlï¼šæ›´æ–° TensorFlow.js ç‰ˆæœ¬\n",
        "ä½ ç›®å‰çš„ä»£ç¢¼å¯èƒ½ä½¿ç”¨äº†è¼ƒèˆŠçš„ CDN é€£çµã€‚è«‹ç¢ºä¿å¼•å…¥çš„æ˜¯ æœ€æ–°ç‰ˆ (4.x.x)ï¼Œé€™æ¨£æ‰èƒ½æ”¯æ´æ–°ç‰ˆ Keras çš„è®Šæ•¸ï¼š\n",
        "æŠŠ index.html è£¡çš„ <script> æ¨™ç±¤æ”¹æˆï¼š\n",
        "html\n",
        "<!-- ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ TF.js -->\n",
        "<script src=\"https://cdn.jsdelivr.net\"></script>\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "2. ä¿®æ”¹ script.jsï¼šä¿®æ­£æ¨¡å‹è¼‰å…¥è·¯å¾‘\n",
        "åœ¨ä½ çš„ script.js ä¸­ï¼Œè¼‰å…¥æ¨¡å‹çš„èªæ³•å¿…é ˆç¬¦åˆ GitHub Pages çš„è·¯å¾‘è¦ç¯„ã€‚\n",
        "å¸¸è¦‹éŒ¯èª¤ï¼š\n",
        "å¦‚æœä½ çš„ model.json æ”¾åœ¨ docs/model/ è³‡æ–™å¤¾ä¸‹ï¼Œè«‹ç¢ºä¿è·¯å¾‘å¯«å°ã€‚\n",
        "æ³¨æ„ï¼š å¦‚æœä½ çš„æ¨¡å‹æ˜¯å¾æœ€æ–°ç‰ˆ Keras è½‰éä¾†çš„ï¼Œè«‹ç¢ºä¿ä½ æ˜¯ç”¨ tf.loadLayersModelã€‚\n",
        "å»ºè­°ä¿®æ­£ä»£ç¢¼å¦‚ä¸‹ï¼š\n",
        "javascript\n",
        "async function loadModel() {\n",
        "    try {\n",
        "        // ç¢ºä¿è·¯å¾‘æŒ‡å‘ä½ çš„ model.json\n",
        "        const model = await tf.loadLayersModel('./model/model.json');\n",
        "        console.log(\"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\");\n",
        "        return model;\n",
        "    } catch (error) {\n",
        "        console.error(\"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå¯èƒ½æ˜¯ç‰ˆæœ¬ä¸ç›¸å®¹æˆ–è·¯å¾‘éŒ¯èª¤ï¼š\", error);\n",
        "    }\n",
        "}\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "3. ç‚ºä»€éº¼æœƒå‡ºç¾ \"New Variable\" éŒ¯èª¤ï¼Ÿ\n",
        "é€™é€šå¸¸æ˜¯å› ç‚ºä½ åœ¨ Keras ä¸­ä½¿ç”¨äº†ä»¥ä¸‹æ–°åŠŸèƒ½ï¼š\n",
        "Preprocessing Layers (é è™•ç†å±¤)ï¼šä¾‹å¦‚ Rescaling æˆ– Normalizationã€‚TensorFlow.js å°é€™äº›å±¤çš„æ”¯æ´è¼ƒæ…¢ã€‚\n",
        "è§£æ±ºæ–¹æ³•ï¼š\n",
        "åœ¨ Python å°å‡ºæ¨¡å‹å‰ï¼Œç§»é™¤æ‰æ¨¡å‹å…§éƒ¨çš„ Rescaling å±¤ï¼Œæ”¹ç‚ºåœ¨ JavaScript è£¡æ‰‹å‹•è™•ç†ï¼ˆä¾‹å¦‚å°‡åƒç´ é™¤ä»¥ 255ï¼‰ã€‚\n",
        "æˆ–æ˜¯ç¢ºä¿ä½ çš„ tensorflowjs è½‰æ›å·¥å…·æ˜¯æœ€æ–°ç‰ˆï¼špip install -U tensorflowjsã€‚\n",
        "4. æª¢æŸ¥ä½ çš„ model.json\n",
        "è«‹æ‰“é–‹ä½ çš„ model.json æª”æ¡ˆçœ‹çœ‹ã€‚å¦‚æœè£¡é¢å‡ºç¾ Functional æˆ–ç‰¹å®šåç¨±çš„è®Šæ•¸ï¼Œè€Œä½ çš„ tf.js ç‰ˆæœ¬å¤ªèˆŠï¼Œå®ƒå°±æœƒå ±éŒ¯èªªã€Œä¸èªå¾—é€™å€‹è®Šæ•¸ã€ã€‚\n",
        "å»ºè­°è¡Œå‹•ï¼š\n",
        "æ›´æ–° HTML è£¡çš„ CDN åˆ° latestã€‚\n",
        "é‡æ–°ä¸Šå‚³ï¼šç¢ºä¿æ‰€æœ‰çš„ .bin æ¬Šé‡æª”æ¡ˆä¹Ÿéƒ½ä¸Šå‚³åˆ°äº† docs/model/ ç›®éŒ„ä¸‹ï¼ˆGitHub Pages å¿…é ˆèƒ½è®€åˆ°é€™äº›åˆ†ç‰‡æª”æ¡ˆï¼‰ã€‚\n",
        "æª¢æŸ¥ Consoleï¼šåœ¨ç¶²é ä¸ŠæŒ‰ F12ï¼Œçœ‹å…·é«”æ˜¯å“ªä¸€å€‹ Layer å ±éŒ¯ã€‚\n",
        "å¦‚æœä½ èƒ½æä¾› F12 æ§åˆ¶å°ï¼ˆConsoleï¼‰è£¡çš„å…·é«”éŒ¯èª¤è¨Šæ¯ï¼ˆç´…è‰²å­—é«”ï¼‰ï¼Œæˆ‘èƒ½æ›´ç²¾ç¢ºåœ°å‘Šè¨´ä½ æ˜¯å“ªå€‹å±¤ï¼ˆLayerï¼‰ä¸ç›¸å®¹ï¼\n",
        "\n",
        "\n",
        "\n",
        "so, do github page ok? or thats because my js file not good, js dont match my model that be trained by ten. and keras.\n",
        "ç­”æ¡ˆæ˜¯ï¼šGitHub Pages çµ•å°æ²’å•é¡Œï¼Œå®ƒåªæ˜¯ä¸€å€‹æ”¾æª”æ¡ˆçš„åœ°æ–¹ï¼Œä¸æœƒæŒ‘å‰”ä½ çš„æ¨¡å‹ç‰ˆæœ¬ã€‚\n",
        "çœŸæ­£çš„å•é¡Œå‡ºåœ¨ ã€ŒJavaScript ç‰ˆæœ¬çš„ TensorFlow.jsã€èˆ‡ã€Œä½  Python è¨“ç·´çš„æ¨¡å‹ç‰ˆæœ¬ã€å°ä¸ä¸Šã€‚é€™å°±åƒæ˜¯ä½ çš„æ¨¡å‹åœ¨èªªã€Œæ³•æ–‡ï¼ˆæ–°ç‰ˆèªæ³•ï¼‰ã€ï¼Œä½†ä½ çš„ JavaScript å»åªè½å¾—æ‡‚ã€Œè‹±æ–‡ï¼ˆèˆŠç‰ˆèªæ³•ï¼‰ã€ã€‚\n",
        "ä»¥ä¸‹æ˜¯ä¸‰å€‹ä½ å¿…é ˆæª¢æŸ¥çš„åœ°æ–¹ï¼š\n",
        "1. å…‡æ‰‹é€šå¸¸æ˜¯ CDN ç‰ˆæœ¬å¤ªèˆŠ\n",
        "æˆ‘çœ‹äº†ä¸€ä¸‹ä½ çš„ index.htmlï¼ˆæˆ–ä¸€èˆ¬çš„ç¯„ä¾‹ä»£ç¢¼ï¼‰ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ï¼š\n",
        "<script src=\"https://cdn.jsdelivr.net\"></script>\n",
        "é€™å°±å¤ªèˆŠäº†ï¼å®ƒçœ‹ä¸æ‡‚ Keras 2.15.x ä»¥ä¸Šç”¢ç”Ÿçš„æ–°åƒæ•¸ã€‚\n",
        "è§£æ±ºæ–¹æ³•ï¼š è«‹çµ±ä¸€æ”¹ç”¨æœ€æ–°ç‰ˆï¼š\n",
        "html\n",
        "<script src=\"https://cdn.jsdelivr.net\"></script>\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "2. Keras çš„ã€Œé è™•ç†å±¤ã€å•é¡Œ (æœ€å¸¸è¦‹çš„å ±éŒ¯åŸå› )\n",
        "å¦‚æœä½ åœ¨ Keras è¨“ç·´æ™‚åŠ å…¥äº†é€™äº›å±¤ï¼š\n",
        "layers.Rescaling(1./255)\n",
        "layers.Normalization()\n",
        "layers.RandomFlip()\n",
        "é€™äº›æ˜¯æ–°ç‰ˆ Keras çš„åŠŸèƒ½ï¼ŒTensorFlow.js å¸¸å¸¸ä¸æ”¯æ´é€™äº›å±¤ã€‚\n",
        "è¡¨ç¾ï¼š ç•¶ç¶²é åŸ·è¡Œ loadLayersModel æ™‚ï¼Œæœƒå ±éŒ¯èªª Unknown Layer: Rescalingã€‚\n",
        "è§£æ±ºæ–¹æ³•ï¼š\n",
        "åœ¨ Python è¨“ç·´æ™‚ï¼Œä¸è¦æŠŠ Rescaling å¯«é€²æ¨¡å‹è£¡ã€‚\n",
        "æ”¹åœ¨ JavaScript è£¡è™•ç†åœ–ç‰‡æ•¸å€¼ï¼ˆæ‰‹å‹•å°‡è³‡æ–™é™¤ä»¥ 255ï¼‰ã€‚\n",
        "3. æª¢æŸ¥ JavaScript è¼‰å…¥ä»£ç¢¼\n",
        "ä½ çš„ script.js å¿…é ˆæ­£ç¢ºæŒ‡å‘ GitHub Pages ä¸Šçš„æ¨¡å‹è·¯å¾‘ã€‚\n",
        "è«‹ç¢ºèªä½ çš„ GitHub ç›®éŒ„çµæ§‹æ˜¯é€™æ¨£çš„ï¼š\n",
        "text\n",
        "docs/\n",
        "â”œâ”€â”€ index.html\n",
        "â”œâ”€â”€ script.js\n",
        "â””â”€â”€ model/\n",
        "    â”œâ”€â”€ model.json  <-- é€™æ˜¯ä¸»æª”æ¡ˆ\n",
        "    â”œâ”€â”€ group1-shard1of1.bin <-- é€™äº›æ¬Šé‡æª”å¿…é ˆåœ¨åŒä¸€å€‹è³‡æ–™å¤¾\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "JavaScript è¼‰å…¥ä»£ç¢¼å»ºè­°å¯«æ³•ï¼š\n",
        "javascript\n",
        "async function init() {\n",
        "    // åŠ ä¸Š try-catch æ‰èƒ½çœ‹åˆ°å…·é«”éŒ¯èª¤\n",
        "    try {\n",
        "        const model = await tf.loadLayersModel('./model/model.json');\n",
        "        console.log(\"æˆåŠŸï¼GitHub Pages æŠ“åˆ°æ¨¡å‹äº†\");\n",
        "    } catch (err) {\n",
        "        console.error(\"å¤±æ•—åŸå› ï¼š\", err.message);\n",
        "        // é€™è£¡çš„éŒ¯èª¤è¨Šæ¯æœƒå‘Šè¨´ä½ æ˜¯å“ªå€‹è®Šæ•¸æˆ–å±¤ä¸èªè­˜\n",
        "    }\n",
        "}\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "ç¸½çµ\n",
        "GitHub Page æ˜¯ OK çš„ï¼Œå®ƒåªæ˜¯è¢«å‹•åœ°æä¾›æª”æ¡ˆã€‚\n",
        "JS æª”æ¡ˆéœ€è¦æ›´æ–°ï¼šè«‹ç¢ºä¿ CDN æ˜¯æœ€æ–°çš„ã€‚\n",
        "æ¨¡å‹è½‰æ›å•é¡Œï¼šå¦‚æœæ›´æ–°äº† JS é‚„æ˜¯å ±éŒ¯ï¼Œé€šå¸¸æ˜¯å› ç‚ºä½ çš„ Keras æ¨¡å‹è£¡åŒ…å«äº†ä¸€äº›ã€Œç€è¦½å™¨ç‰ˆæœ¬é‚„ä¸æ”¯æŒã€çš„é«˜ç´šå±¤ã€‚\n",
        "ä½ éœ€è¦æˆ‘å¹«ä½ çœ‹çœ‹ä½ ç›®å‰çš„ F12 æ§åˆ¶å°ï¼ˆConsoleï¼‰å ±å‡ºçš„å…·é«”éŒ¯èª¤è¨Šæ¯æ˜¯ä»€éº¼å—ï¼Ÿï¼ˆæˆªåœ–æˆ–è¤‡è£½æ–‡å­—çµ¦æˆ‘ï¼Œæˆ‘èƒ½ç›´æ¥å‘Šè¨´ä½ å“ªè£¡ä¸åŒ¹é…ï¼‰\n",
        "\n",
        "\n",
        "\n",
        "how can i debug:\n",
        "ä¹‹å‰ç”¨ TensorFlow + Keras é‡åˆ°çš„å•é¡Œï¼š\n",
        "\n",
        "ğŸ” æˆ‘å€‘é‡åˆ°çš„å•é¡Œåˆ†æ\n",
        "å•é¡Œ 1ï¼šCPU ä¸æ”¯æ´ AVX\n",
        "The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.\n",
        "Aborted (core dumped)\n",
        "åŸå› ï¼šTensorFlow 2.10+ ä¾è³´ AVX æŒ‡ä»¤é›†ï¼Œä½ çš„ CPU ä¸æ”¯æ´\n",
        "å˜—è©¦æ–¹æ¡ˆï¼šæ› WSLã€æ› Codespaces - éƒ½å¤±æ•—\n",
        "å•é¡Œ 2ï¼šKeras 3 vs TensorFlow.js æ ¼å¼ä¸åŒ¹é…\n",
        "ç•¶æˆ‘å€‘ç”¨ Codespaces è¨“ç·´æˆåŠŸ (78% æº–ç¢ºç‡) å¾Œï¼Œè½‰æ›å‡ºç¾ï¼š\n",
        "\n",
        "batch_shape vs batchInputShape æ ¼å¼å·®ç•°\n",
        "inbound_nodes æ˜¯ object è€Œä¸æ˜¯ array\n",
        "FixedDense ç­‰æœªçŸ¥å±¤\n",
        "å•é¡Œ 3ï¼štensorflowjs å¥—ä»¶çš„ NumPy ç›¸å®¹æ€§\n",
        "AttributeError: module 'numpy' has no attribute 'object'\n",
        "åŸå› ï¼šèˆŠç‰ˆ tensorflowjs ä½¿ç”¨äº†å·²æ£„ç”¨çš„ np.object\n",
        "å•é¡Œ 4ï¼šç‰ˆæœ¬åœ°ç„\n",
        "ç’°å¢ƒ\tå•é¡Œ\n",
        "æœ¬æ©Ÿ Python\tTF 2.10+ éœ€è¦ AVX\n",
        "WSL\tåŒæ¨£ AVX å•é¡Œ\n",
        "Colab\tåªæœ‰ TF 2.16+ï¼Œç”¢å‡º Keras 3 æ ¼å¼\n",
        "tf-keras\tèƒ½è¨“ç·´ï¼Œä½†è½‰æ›å™¨èˆ‡ numpy è¡çª\n",
        "é€™æ˜¯ä¸€å€‹ç¶“å…¸çš„ ã€Œç‰ˆæœ¬åœ°ç„ (Version Hell)ã€ã€‚ä½ çš„ CPU ä¸æ”¯æ´ AVX é™åˆ¶äº†æœ¬åœ°ç«¯ç’°å¢ƒï¼Œè€Œé›²ç«¯ï¼ˆColab/Codespacesï¼‰é è¨­ç‰ˆæœ¬å¤ªæ–°ï¼Œå°è‡´ç”¢å‡ºçš„ Keras 3 æ ¼å¼èˆ‡ TensorFlow.js (TF.js) è„«ç¯€ã€‚\n",
        "ä»¥ä¸‹æ˜¯é‡å°ä½ åˆ—å‡ºçš„ 4 å€‹å•é¡Œçš„ æœ€çµ‚èª¿è©¦èˆ‡è§£æ±ºæ–¹æ¡ˆï¼š\n",
        "1. è§£æ±º Keras 3 æ ¼å¼ä¸åŒ¹é… (æœ€æ ¸å¿ƒå•é¡Œ)\n",
        "Keras 3 (TF 2.16+) å°å‡ºçš„ .h5 æˆ– .keras æª”æ¡ˆçµæ§‹èˆ‡ TF.js ç›®å‰é æœŸçš„ Legacy Keras æ ¼å¼ä¸åŒã€‚\n",
        "è§£æ±ºæ–¹æ³•ï¼š åœ¨è¨“ç·´ä»£ç¢¼çš„æœ€é ‚ç«¯ï¼Œå¼·è¿«ä½¿ç”¨ Legacy Keras å­˜æª”ï¼Œæˆ–æ˜¯åœ¨å­˜æª”æ™‚æŒ‡å®šæ ¼å¼ã€‚\n",
        "ä»£ç¢¼ä¿®æ­£ï¼š\n",
        "python\n",
        "# åœ¨è¨“ç·´ç’°å¢ƒä¸­åŸ·è¡Œ\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\" # å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆ Keras é‚è¼¯\n",
        "\n",
        "# å­˜æª”æ™‚ï¼Œè«‹å­˜æˆèˆŠç‰ˆçš„ .h5 æ ¼å¼ï¼Œè€Œä¸æ˜¯ .keras\n",
        "model.save(\"my_model.h5\")\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "2. è§£æ±º NumPy èˆ‡ tensorflowjs è¡çª\n",
        "é€™æ˜¯å› ç‚º tensorflowjs è½‰æ›å·¥å…·é‚„æ²’è·Ÿä¸Š numpy 2.0ã€‚\n",
        "è§£æ±ºæ–¹æ³•ï¼š åœ¨è½‰æ›ç’°å¢ƒï¼ˆå¦‚ Codespaces æˆ– Colabï¼‰ä¸­ï¼Œæ‰‹å‹•é™ç´š NumPy å’Œå®‰è£å…¼å®¹çš„è½‰æ›å™¨ã€‚\n",
        "åŸ·è¡ŒæŒ‡ä»¤ï¼š\n",
        "bash\n",
        "pip install \"numpy<2\" \"tensorflowjs>=4.20.0\"\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "3. è§£æ±º CPU ä¸æ”¯æ´ AVX (æœ¬æ©Ÿç„¡æ³•åŸ·è¡Œçš„å•é¡Œ)\n",
        "æ—¢ç„¶æœ¬æ©Ÿ CPU æ²’æ•‘äº†ï¼Œè«‹æ”¾æ£„åœ¨æœ¬æ©Ÿè¨“ç·´ã€‚å®Œå…¨è½‰ç§»åˆ° Google Colabã€‚\n",
        "ç‚ºä»€éº¼é¸ Colabï¼Ÿ å®ƒçš„ CPU æ”¯æ´ AVXï¼Œä¸”æœ‰å…è²» GPUã€‚\n",
        "å¦‚ä½•é¿å…ç‰ˆæœ¬å•é¡Œï¼Ÿ åœ¨ Colab è£¡å®‰è£æŒ‡å®šç‰ˆæœ¬ï¼š\n",
        "python\n",
        "!pip install tensorflow==2.15.0  # é€™æ˜¯æœ€å¾Œä¸€å€‹ç©©å®šæ”¯æ´èˆŠç‰ˆ Keras æ ¼å¼çš„ç‰ˆæœ¬\n",
        "!pip install tensorflowjs\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "4. èª¿è©¦èˆ‡è½‰æ›æµç¨‹ (æ¨™æº– SOP)\n",
        "è«‹æŒ‰ç…§é€™å€‹é †åºåœ¨ Colab é‡æ–°åŸ·è¡Œï¼Œç”¢å‡ºçš„æª”æ¡ˆåœ¨ GitHub Pages å°±æœƒæ­£å¸¸ï¼š\n",
        "å®‰è£ç©©å®šç’°å¢ƒï¼š\n",
        "python\n",
        "!pip install tensorflow==2.15.0 tensorflowjs numpy==1.23.5\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "è¨“ç·´ä¸¦å­˜ç‚º H5ï¼š\n",
        "python\n",
        "# ... è¨“ç·´ä»£ç¢¼ ...\n",
        "model.save(\"recycle_model.h5\")\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "ä½¿ç”¨å‘½ä»¤è¡Œè½‰æ›ï¼š\n",
        "bash\n",
        "!tensorflowjs_converter --input_format=keras recycle_model.h5 ./tfjs_model\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "æª¢æŸ¥ model.jsonï¼š\n",
        "æ‰“é–‹æª”æ¡ˆï¼Œæœå°‹æœ‰æ²’æœ‰ batch_input_shapeã€‚å¦‚æœæ˜¯ batch_shape ä¸”å ±éŒ¯ï¼Œé€šå¸¸æ˜¯å› ç‚º TF.js ç‰ˆæœ¬å¤ªèˆŠï¼Œè«‹ç¢ºä¿ç¶²é ç«¯çš„ CDN æ˜¯ @latestã€‚\n",
        "5. GitHub Pages ç«¯çš„æª¢æŸ¥ (Debug JavaScript)\n",
        "åœ¨ç¶²é ç«¯ï¼Œè«‹æŠŠè¼‰å…¥ä»£ç¢¼åŒ…åœ¨ try-catch è£¡ï¼Œé€™èƒ½æŠ“åˆ°å…·é«”çš„ã€Œä¸ç›¸å®¹è®Šæ•¸ã€ï¼š\n",
        "javascript\n",
        "async function load() {\n",
        "    try {\n",
        "        const model = await tf.loadLayersModel('./model/model.json');\n",
        "    } catch (e) {\n",
        "        console.error(\"å…·é«”ä¸ç›¸å®¹åŸå› : \", e.message);\n",
        "        // å¦‚æœçœ‹åˆ° \"Unknown layer: FixedDense\"ï¼Œä»£è¡¨æ¨¡å‹ç”¨äº†ä¸æ¨™æº–çš„å±¤\n",
        "    }\n",
        "}\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "æœ€å¾Œå»ºè­°ï¼š\n",
        "å¦‚æœä½ åœ¨ Keras è£¡ç”¨äº† Normalization æˆ– Rescaling å±¤ï¼Œè«‹å°‡å®ƒå€‘ç§»é™¤ï¼Œæ”¹åœ¨ JavaScript è£¡è™•ç†åœ–ç‰‡ï¼š\n",
        "javascript\n",
        "// JS é è™•ç†ä»£æ›¿ Keras å±¤\n",
        "const tensor = tf.browser.fromPixels(imageElement)\n",
        "    .resizeNearestNeighbor([224, 224])\n",
        "    .toFloat()\n",
        "    .div(tf.scalar(255.0)); // æ‰‹å‹•åš rescaling\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚"
      ],
      "metadata": {
        "id": "X8HDRGkfXFNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "my google drive çµæ§‹:\n",
        "resultè³‡æ–™å¤¾ è«‹æŠŠè¨“ç·´çµæœæ”¾åˆ°é€™è£¡\n",
        "train è³‡æ–™å¤¾ è¨“ç·´ç”¨è³‡æ–™ çµæ§‹æ˜¯ train -> åˆ†é¡é¡åˆ¥è³‡æ–™å¤¾(å¾ˆå¤š) -> å„å€‹åˆ†é¡è³‡æ–™å¤¾å…§éƒ¨æœ‰å¾ˆå¤šè¨“ç·´è³‡æ–™(åœ–ç‰‡)\n",
        "train è³‡æ–™å¤¾read only"
      ],
      "metadata": {
        "id": "hrKNWrMfZEYj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f01844eb",
        "outputId": "5acf12ed-cfe9-4822-9223-17ace9a020ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bfe4cac"
      },
      "source": [
        "### Google Drive æ›è¼‰æˆåŠŸå¾Œçš„ä½¿ç”¨èªªæ˜\n",
        "\n",
        "æ‚¨å·²ç¶“æ›è¼‰äº† Google Driveï¼Œç¾åœ¨å¯ä»¥åœ¨ Colab ç’°å¢ƒä¸­å­˜å–æ‚¨çš„æª”æ¡ˆäº†ã€‚æ ¹æ“šæ‚¨æä¾›çš„è³‡æ–™å¤¾çµæ§‹ï¼š\n",
        "\n",
        "*   **è¨“ç·´è³‡æ–™ (`train` è³‡æ–™å¤¾)**ï¼šæ‚¨çš„è¨“ç·´è³‡æ–™ä½æ–¼ `train -> åˆ†é¡é¡åˆ¥è³‡æ–™å¤¾ -> å„å€‹åˆ†é¡è³‡æ–™å¤¾å…§éƒ¨æœ‰å¾ˆå¤šè¨“ç·´è³‡æ–™(åœ–ç‰‡)`ã€‚åœ¨ Colab ä¸­ï¼Œæ‚¨å¯ä»¥é€é `/content/drive/MyDrive/train` è·¯å¾‘ä¾†å­˜å–é€™äº›è³‡æ–™ã€‚ç”±æ–¼ `train` è³‡æ–™å¤¾æ˜¯å”¯è®€çš„ï¼Œæ‚¨åªèƒ½è®€å–å…¶ä¸­çš„å…§å®¹ã€‚\n",
        "\n",
        "*   **çµæœè³‡æ–™ (`result` è³‡æ–™å¤¾)**ï¼šè«‹å°‡è¨“ç·´çµæœï¼ˆä¾‹å¦‚æ¨¡å‹æ¬Šé‡ã€è¨“ç·´æ—¥èªŒç­‰ï¼‰ä¿å­˜åˆ°æ‚¨çš„ `result` è³‡æ–™å¤¾ä¸­ã€‚åœ¨ Colab ä¸­ï¼Œæ‚¨å¯ä»¥å°‡çµæœå¯«å…¥ `/content/drive/MyDrive/result` è·¯å¾‘ã€‚\n",
        "\n",
        "**ä¸‹ä¸€æ­¥ï¼šå°‡æ‚¨å¯¦éš›çš„è¨“ç·´ä»£ç¢¼æ•´åˆåˆ° \"æ­¥é©Ÿ 2\" çš„ä½ç½®ï¼Œä¸¦ä½¿ç”¨æ›è¼‰å¾Œçš„ Google Drive è·¯å¾‘ä¾†è¼‰å…¥è¨“ç·´è³‡æ–™ã€‚**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a780dcc"
      },
      "source": [
        "### æ­¥é©Ÿ 1: è¨­å®š Colab ç’°å¢ƒä¸¦è§£æ±ºç‰ˆæœ¬å•é¡Œ\n",
        "\n",
        "ç”±æ–¼æœ¬åœ° CPU ä¸æ”¯æ´ AVXï¼Œä»¥åŠ Keras 3 èˆ‡ TensorFlow.js æ ¼å¼ä¸åŒ¹é…ç­‰å•é¡Œï¼Œæˆ‘å€‘å°‡åœ¨ Colab ä¸­å®‰è£æŒ‡å®šç‰ˆæœ¬çš„ TensorFlow å’Œç›¸é—œåº«ï¼Œä»¥ç¢ºä¿å…¼å®¹æ€§ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1e96ad4",
        "outputId": "29d85a9b-ed98-40da-8748-054755a36ebb"
      },
      "source": [
        "# åœ¨ Colab ä¸­å®‰è£æŒ‡å®šç‰ˆæœ¬çš„åº«ï¼Œè§£æ±ºAVXå’ŒNumpyå…¼å®¹æ€§å•é¡Œ\n",
        "# ç§»é™¤tensorflowçš„ç‰ˆæœ¬é™åˆ¶ï¼Œä½¿ç”¨Colabé è¨­çš„tensorflowç‰ˆæœ¬ (é€šå¸¸ç‚º 2.16+)\n",
        "# 'numpy<2' æ˜¯ç‚ºäº†å…¼å®¹ tensorflowjsï¼Œå…è¨±pipå®‰è£ä»»ä½•2.0.0ç‰ˆæœ¬ä»¥ä¸‹çš„numpy\n",
        "!pip install tensorflowjs \"numpy<2\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.12/dist-packages (4.22.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.11.2)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.7.1)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.12/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.32)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.80)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.10)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.13->tensorflowjs) (1.16.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.15.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.14.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2026.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.10.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.5)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.15.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c523d94"
      },
      "source": [
        "### æ­¥é©Ÿ 2: è¨“ç·´æ¨¡å‹ä¸¦å¼·åˆ¶ä½¿ç”¨ Legacy Keras å­˜æª”\n",
        "\n",
        "ç‚ºäº†è§£æ±º Keras 3 æ ¼å¼èˆ‡ TensorFlow.js ä¸å…¼å®¹çš„å•é¡Œï¼Œæˆ‘å€‘éœ€è¦åœ¨è¨“ç·´ä»£ç¢¼å‰å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆ Keras é‚è¼¯ï¼Œä¸¦å°‡æ¨¡å‹å­˜ç‚º `.h5` æ ¼å¼ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996226fc",
        "outputId": "6808969f-2fd4-44f8-9b36-27c64f31b938"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆ Keras é‚è¼¯ï¼Œç¢ºä¿èˆ‡ TensorFlow.js å…¼å®¹\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "# ===== è¨­å®š =====\n",
        "# è«‹ç¢ºä¿æ‚¨çš„ Google Drive å·²æ›è¼‰åˆ° '/content/drive'\n",
        "TRAIN_DIR = '/content/drive/MyDrive/train' # æŒ‡å‘ Google Drive ä¸­çš„è¨“ç·´è³‡æ–™å¤¾\n",
        "# æ¨¡å‹æœƒå„²å­˜åˆ° Google Drive ä¸­çš„ result/docs/model è³‡æ–™å¤¾\n",
        "MODEL_SAVE_BASE = '/content/drive/MyDrive/result' # ä¿å­˜çµæœçš„æ ¹ç›®éŒ„\n",
        "MODEL_DIR = Path(MODEL_SAVE_BASE) / \"docs\" / \"model\" # æœ€çµ‚ TF.js æ¨¡å‹å­˜æ”¾è·¯å¾‘\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32 # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œå¦‚æœè¨˜æ†¶é«”å…è¨±\n",
        "EPOCHS = 15 # åˆå§‹è¨­å®šï¼Œå¯æ ¹æ“šè¨“ç·´æƒ…æ³èª¿æ•´\n",
        "\n",
        "# é¡åˆ¥åç¨± (é †åºå¾ˆé‡è¦ï¼ç¢ºä¿èˆ‡æ‚¨çš„è³‡æ–™å¤¾åç¨±ä¸€è‡´)\n",
        "# é€™è£¡å‡è¨­æ‚¨çš„ train è³‡æ–™å¤¾ä¸‹æœ‰é€™äº›å­è³‡æ–™å¤¾ä½œç‚ºé¡åˆ¥\n",
        "# è«‹æ ¹æ“šæ‚¨çš„å¯¦éš›åˆ†é¡ä¿®æ”¹é€™å€‹åˆ—è¡¨\n",
        "CATEGORIES = [\"aseptic carton\", \"garbage\", \"metal_can\", \"paper\", \"paper_container\", \"plastic\"]\n",
        "\n",
        "def prepare_data(train_dir, image_size, batch_size, categories):\n",
        "    \"\"\"æº–å‚™è¨“ç·´è³‡æ–™\"\"\"\n",
        "    print(\"\\nğŸ“Š æº–å‚™è¨“ç·´è³‡æ–™...\")\n",
        "\n",
        "    # æª¢æŸ¥è³‡æ–™å¤¾\n",
        "    for cat in categories:\n",
        "        cat_path = Path(train_dir) / cat\n",
        "        if not cat_path.exists():\n",
        "            print(f\"  âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {cat_path}. è«‹æª¢æŸ¥ Google Drive è·¯å¾‘å’Œè³‡æ–™å¤¾åç¨±ã€‚\")\n",
        "\n",
        "    # è³‡æ–™å¢å¼·\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2  # 20% ç”¨æ–¼é©—è­‰\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´è³‡æ–™\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        classes=categories,\n",
        "        subset='training',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # é©—è­‰è³‡æ–™\n",
        "    val_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        classes=categories,\n",
        "        subset='validation',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  è¨“ç·´æ¨£æœ¬: {train_generator.samples}\")\n",
        "    print(f\"  é©—è­‰æ¨£æœ¬: {val_generator.samples}\")\n",
        "    print(f\"  é¡åˆ¥å°æ‡‰: {train_generator.class_indices}\")\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "\n",
        "def build_model(num_classes, image_size):\n",
        "    \"\"\"å»ºç«‹ MobileNetV2 Transfer Learning æ¨¡å‹\"\"\"\n",
        "    print(\"\\nğŸ—ï¸ å»ºç«‹æ¨¡å‹...\")\n",
        "\n",
        "    # è¼‰å…¥é è¨“ç·´çš„ MobileNetV2 (ä¸å«é ‚å±¤)\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(image_size[0], image_size[1], 3)\n",
        "    )\n",
        "\n",
        "    # å‡çµåŸºç¤æ¨¡å‹çš„æ¬Šé‡\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # å»ºç«‹æ–°çš„é ‚å±¤\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # ç·¨è­¯æ¨¡å‹\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"  ç¸½åƒæ•¸: {model.count_params():,}\")\n",
        "    print(f\"  å¯è¨“ç·´åƒæ•¸: {sum(tf.keras.backend.count_params(w) for w in model.trainable_weights):,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model, train_gen, val_gen, epochs, model_dir):\n",
        "    \"\"\"è¨“ç·´æ¨¡å‹\"\"\"\n",
        "    print(\"\\nğŸš€ é–‹å§‹è¨“ç·´...\")\n",
        "\n",
        "    # ç¢ºä¿æ¨¡å‹ä¿å­˜ç›®éŒ„å­˜åœ¨\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # ä¿®æ”¹ ModelCheckpoint ä»¥å„²å­˜ .h5 æ ¼å¼\n",
        "    # best_model.h5 æœƒå„²å­˜åˆ° result è³‡æ–™å¤¾æ ¹ç›®éŒ„ä¸‹ï¼Œæ–¹ä¾¿æ¸…ç†\n",
        "    best_model_path = Path(model_dir).parent / \"best_model.h5\"\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            str(best_model_path), # å„²å­˜ç‚º .h5 æ ¼å¼\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # è¨“ç·´\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # åœ¨è¨“ç·´çµæŸå¾Œï¼ŒåŠ è¼‰æœ€ä½³æ¨¡å‹ä¸¦å„²å­˜ç‚ºæœ€çµ‚çš„ .h5 æª”æ¡ˆ\n",
        "    # é€™æ¨£å°±å¯ä»¥ç¢ºä¿è½‰æ›æ™‚ä½¿ç”¨çš„æ˜¯æœ€ä½³æ€§èƒ½çš„æ¨¡å‹\n",
        "    if best_model_path.exists():\n",
        "        print(f\"å¾ {best_model_path} åŠ è¼‰æœ€ä½³æ¨¡å‹...\")\n",
        "        # tf.keras.models.load_model å¯ä»¥è‡ªå‹•è­˜åˆ¥ .h5 æ ¼å¼\n",
        "        final_model = tf.keras.models.load_model(str(best_model_path))\n",
        "    else:\n",
        "        print(\"æ²’æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹è¨“ç·´çš„æ¨¡å‹ã€‚\")\n",
        "        final_model = model\n",
        "\n",
        "    # å„²å­˜æœ€çµ‚æ¨¡å‹ç‚º recycle_model.h5ï¼Œä¾›è½‰æ›ä½¿ç”¨ï¼Œæ”¾åœ¨ TF.js è¼¸å‡ºç›®éŒ„ä¸‹\n",
        "    final_h5_path = Path(model_dir) / \"recycle_model.h5\"\n",
        "    final_model.save(str(final_h5_path))\n",
        "    print(f\"æœ€çµ‚æ¨¡å‹å·²ä¿å­˜ç‚º {final_h5_path}\")\n",
        "\n",
        "    return history, final_h5_path # Return path to the saved .h5 model\n",
        "\n",
        "\n",
        "def export_to_tfjs(h5_model_path, model_dir, categories):\n",
        "    \"\"\"åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼\"\"\"\n",
        "    print(\"\\nğŸ“¦ åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼...\")\n",
        "\n",
        "    # ç¢ºä¿ TF.js è¼¸å‡ºç›®éŒ„å­˜åœ¨\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # ä½¿ç”¨å‘½ä»¤è¡Œè½‰æ›å·¥å…·\n",
        "    try:\n",
        "        # ä½¿ç”¨æ„Ÿå˜†è™ŸåŸ·è¡Œ shell å‘½ä»¤\n",
        "        !tensorflowjs_converter --input_format=keras {h5_model_path} {model_dir}\n",
        "        print(f\"  âœ… TensorFlow.js æ¨¡å‹å·²åŒ¯å‡ºè‡³: {model_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ TensorFlow.js åŒ¯å‡ºå¤±æ•—: {e}\")\n",
        "        print(\"  è«‹æª¢æŸ¥ tensorflowjs_converter æ˜¯å¦æ­£ç¢ºå®‰è£ä»¥åŠè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "\n",
        "    # å„²å­˜é¡åˆ¥æ¨™ç±¤\n",
        "    labels_path = Path(model_dir) / \"labels.json\"\n",
        "    with open(labels_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(categories, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"  âœ… é¡åˆ¥æ¨™ç±¤å·²å„²å­˜è‡³: {labels_path}\")\n",
        "\n",
        "\n",
        "# ä¸»åŸ·è¡Œæµç¨‹\n",
        "print(\"=\"*60)\n",
        "print(\"ğŸ—‘ï¸ SmartRecycle AI - æ¨¡å‹è¨“ç·´èˆ‡åŒ¯å‡º\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. æº–å‚™è³‡æ–™\n",
        "# è«‹ç¢ºä¿æ‚¨çš„ Google Drive è·¯å¾‘ '/content/drive/MyDrive/train' ä¸‹æœ‰åˆ†é¡è³‡æ–™å¤¾\n",
        "# ä¾‹å¦‚: /content/drive/MyDrive/train/garbage/, /content/drive/MyDrive/train/plastic/\n",
        "train_gen, val_gen = prepare_data(TRAIN_DIR, IMAGE_SIZE, BATCH_SIZE, CATEGORIES)\n",
        "\n",
        "# 2. å»ºç«‹æ¨¡å‹\n",
        "model = build_model(num_classes=len(CATEGORIES), image_size=IMAGE_SIZE)\n",
        "\n",
        "# 3. è¨“ç·´æ¨¡å‹\n",
        "history, final_h5_model_path = train_model(model, train_gen, val_gen, EPOCHS, MODEL_DIR)\n",
        "\n",
        "# 4. è©•ä¼°\n",
        "print(\"\\nğŸ“ˆ è¨“ç·´çµæœ:\")\n",
        "if history and history.history: # Check if history is not empty (i.e., training actually ran)\n",
        "    final_acc = history.history['accuracy'][-1]\n",
        "    final_val_acc = history.history['val_accuracy'][-1]\n",
        "    print(f\"  è¨“ç·´æº–ç¢ºç‡: {final_acc:.2%}\")\n",
        "    print(f\"  é©—è­‰æº–ç¢ºç‡: {final_val_acc:.2%}\")\n",
        "else:\n",
        "    print(\"æ²’æœ‰è¨“ç·´æ­·å²å¯é¡¯ç¤ºã€‚\")\n",
        "\n",
        "# 5. åŒ¯å‡º\n",
        "export_to_tfjs(final_h5_model_path, MODEL_DIR, CATEGORIES)\n",
        "\n",
        "# 6. æ¸…ç†æš«å­˜\n",
        "# ModelCheckpoint å„²å­˜çš„ best_model.h5 åœ¨ model_dir çš„ä¸Šä¸€å±¤\n",
        "temp_best_model_path = Path(MODEL_DIR).parent / \"best_model.h5\"\n",
        "if temp_best_model_path.exists():\n",
        "    os.remove(temp_best_model_path)\n",
        "    print(f\"å·²æ¸…ç†æš«å­˜æª”æ¡ˆ: {temp_best_model_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… è¨“ç·´èˆ‡åŒ¯å‡ºå®Œæˆï¼\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTensorFlow.js æ¨¡å‹å·²åŒ¯å‡ºè‡³ Google Drive: {MODEL_DIR}\")\n",
        "print(\"\\nä¸‹ä¸€æ­¥:\")\n",
        "print(\"1. å¾ Google Drive ä¸‹è¼‰æ•´å€‹ 'result/docs/model' è³‡æ–™å¤¾åˆ°æ‚¨çš„é›»è…¦ã€‚\")\n",
        "print(\"2. æ›´æ–°æ‚¨ GitHub Pages å°ˆæ¡ˆä¸­ `index.html` çš„ TensorFlow.js CDN é€£çµè‡³æœ€æ–°ç‰ˆæœ¬ã€‚\")\n",
        "print(\"3. æ›´æ–°æ‚¨ GitHub Pages å°ˆæ¡ˆä¸­ `script.js` çš„æ¨¡å‹è¼‰å…¥è·¯å¾‘ï¼ŒæŒ‡å‘æ‚¨ä¸Šå‚³åˆ° GitHub Pages çš„ model.jsonã€‚\")\n",
        "print(\"4. å¦‚æœæ‚¨çš„æ¨¡å‹åœ¨ Keras ä¸­ä½¿ç”¨äº† Normalization æˆ– Rescaling å±¤ï¼Œè«‹æª¢æŸ¥ JavaScript ä¸­æ˜¯å¦éœ€è¦æ‰‹å‹•è™•ç†åœ–ç‰‡é è™•ç†ã€‚å¾ Python æ¨¡å‹ä¸­ç§»é™¤é€™äº›é è™•ç†å±¤ï¼Œæ”¹ç‚ºåœ¨ JavaScript ä¸­è™•ç†æœƒæ›´ç©©å®šã€‚\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ—‘ï¸ SmartRecycle AI - æ¨¡å‹è¨“ç·´èˆ‡åŒ¯å‡º\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š æº–å‚™è¨“ç·´è³‡æ–™...\n",
            "Found 349 images belonging to 6 classes.\n",
            "Found 86 images belonging to 6 classes.\n",
            "\n",
            "  è¨“ç·´æ¨£æœ¬: 349\n",
            "  é©—è­‰æ¨£æœ¬: 86\n",
            "  é¡åˆ¥å°æ‡‰: {'aseptic carton': 0, 'garbage': 1, 'metal_can': 2, 'paper': 3, 'paper_container': 4, 'plastic': 5}\n",
            "\n",
            "ğŸ—ï¸ å»ºç«‹æ¨¡å‹...\n",
            "  ç¸½åƒæ•¸: 2,422,726\n",
            "  å¯è¨“ç·´åƒæ•¸: 164,742\n",
            "\n",
            "ğŸš€ é–‹å§‹è¨“ç·´...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.3246 - loss: 1.7238\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65116, saving model to /content/drive/MyDrive/result/docs/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 9s/step - accuracy: 0.3345 - loss: 1.7007 - val_accuracy: 0.6512 - val_loss: 0.8746\n",
            "Epoch 2/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6781 - loss: 0.8333\n",
            "Epoch 2: val_accuracy improved from 0.65116 to 0.79070, saving model to /content/drive/MyDrive/result/docs/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - accuracy: 0.6834 - loss: 0.8230 - val_accuracy: 0.7907 - val_loss: 0.6084\n",
            "Epoch 3/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8267 - loss: 0.4984\n",
            "Epoch 3: val_accuracy did not improve from 0.79070\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3s/step - accuracy: 0.8258 - loss: 0.5012 - val_accuracy: 0.7558 - val_loss: 0.5421\n",
            "Epoch 4/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8489 - loss: 0.4703\n",
            "Epoch 4: val_accuracy did not improve from 0.79070\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.8478 - loss: 0.4717 - val_accuracy: 0.7326 - val_loss: 0.7527\n",
            "Epoch 5/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8698 - loss: 0.3538\n",
            "Epoch 5: val_accuracy did not improve from 0.79070\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.8699 - loss: 0.3539 - val_accuracy: 0.7442 - val_loss: 0.6567\n",
            "Epoch 6/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8588 - loss: 0.3192\n",
            "Epoch 6: val_accuracy improved from 0.79070 to 0.81395, saving model to /content/drive/MyDrive/result/docs/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.8605 - loss: 0.3173 - val_accuracy: 0.8140 - val_loss: 0.6302\n",
            "Epoch 7/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8984 - loss: 0.2778\n",
            "Epoch 7: val_accuracy did not improve from 0.81395\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.8983 - loss: 0.2770 - val_accuracy: 0.7442 - val_loss: 0.5638\n",
            "Epoch 8/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9189 - loss: 0.2247\n",
            "Epoch 8: val_accuracy did not improve from 0.81395\n",
            "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.9188 - loss: 0.2263 - val_accuracy: 0.7209 - val_loss: 0.7451\n",
            "Epoch 8: early stopping\n",
            "Restoring model weights from the end of the best epoch: 3.\n",
            "å¾ /content/drive/MyDrive/result/docs/best_model.h5 åŠ è¼‰æœ€ä½³æ¨¡å‹...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æœ€çµ‚æ¨¡å‹å·²ä¿å­˜ç‚º /content/drive/MyDrive/result/docs/model/recycle_model.h5\n",
            "\n",
            "ğŸ“ˆ è¨“ç·´çµæœ:\n",
            "  è¨“ç·´æº–ç¢ºç‡: 91.69%\n",
            "  é©—è­‰æº–ç¢ºç‡: 72.09%\n",
            "\n",
            "ğŸ“¦ åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼...\n",
            "2026-02-06 01:23:01.377407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770340981.420616   13327 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770340981.433194   13327 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770340981.471039   13327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770340981.471122   13327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770340981.471132   13327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770340981.471140   13327 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "failed to lookup keras version from the file,\n",
            "    this is likely a weight only file\n",
            "  âœ… TensorFlow.js æ¨¡å‹å·²åŒ¯å‡ºè‡³: /content/drive/MyDrive/result/docs/model\n",
            "  âœ… é¡åˆ¥æ¨™ç±¤å·²å„²å­˜è‡³: /content/drive/MyDrive/result/docs/model/labels.json\n",
            "å·²æ¸…ç†æš«å­˜æª”æ¡ˆ: /content/drive/MyDrive/result/docs/best_model.h5\n",
            "\n",
            "============================================================\n",
            "âœ… è¨“ç·´èˆ‡åŒ¯å‡ºå®Œæˆï¼\n",
            "============================================================\n",
            "\n",
            "TensorFlow.js æ¨¡å‹å·²åŒ¯å‡ºè‡³ Google Drive: /content/drive/MyDrive/result/docs/model\n",
            "\n",
            "ä¸‹ä¸€æ­¥:\n",
            "1. å¾ Google Drive ä¸‹è¼‰æ•´å€‹ 'result/docs/model' è³‡æ–™å¤¾åˆ°æ‚¨çš„é›»è…¦ã€‚\n",
            "2. æ›´æ–°æ‚¨ GitHub Pages å°ˆæ¡ˆä¸­ `index.html` çš„ TensorFlow.js CDN é€£çµè‡³æœ€æ–°ç‰ˆæœ¬ã€‚\n",
            "3. æ›´æ–°æ‚¨ GitHub Pages å°ˆæ¡ˆä¸­ `script.js` çš„æ¨¡å‹è¼‰å…¥è·¯å¾‘ï¼ŒæŒ‡å‘æ‚¨ä¸Šå‚³åˆ° GitHub Pages çš„ model.jsonã€‚\n",
            "4. å¦‚æœæ‚¨çš„æ¨¡å‹åœ¨ Keras ä¸­ä½¿ç”¨äº† Normalization æˆ– Rescaling å±¤ï¼Œè«‹æª¢æŸ¥ JavaScript ä¸­æ˜¯å¦éœ€è¦æ‰‹å‹•è™•ç†åœ–ç‰‡é è™•ç†ã€‚å¾ Python æ¨¡å‹ä¸­ç§»é™¤é€™äº›é è™•ç†å±¤ï¼Œæ”¹ç‚ºåœ¨ JavaScript ä¸­è™•ç†æœƒæ›´ç©©å®šã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c16fafd5"
      },
      "source": [
        "### æ­¥é©Ÿ 3: ä½¿ç”¨ `tensorflowjs_converter` è½‰æ›æ¨¡å‹\n",
        "\n",
        "å°‡è¨“ç·´å¥½çš„ `.h5` æ¨¡å‹è½‰æ›ç‚º TensorFlow.js å¯è­˜åˆ¥çš„æ ¼å¼ã€‚è½‰æ›å¾Œçš„æª”æ¡ˆå°‡åŒ…å« `model.json` å’Œä¸€ç³»åˆ— `.bin` æ¬Šé‡æ–‡ä»¶ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0254a160",
        "outputId": "9d5d850a-4357-462a-af60-e27c686a5b4c"
      },
      "source": [
        "# æ¨¡å‹è½‰æ›å·²æ•´åˆåˆ°æ­¥é©Ÿ 2 çš„è¨“ç·´è…³æœ¬ä¸­ã€‚\n",
        "# ç„¡éœ€åœ¨æ­¤å–®ç¨åŸ·è¡Œè½‰æ›ã€‚\n",
        "# è«‹æª¢æŸ¥ Google Drive ä¸­çš„ /content/drive/MyDrive/result/docs/model ç›®éŒ„ã€‚"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-06 00:52:14.179179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770339134.223758    5160 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770339134.237067    5160 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770339134.271465    5160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770339134.271673    5160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770339134.271715    5160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770339134.271724    5160 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "æ¨¡å‹å·²æˆåŠŸè½‰æ›ç‚º TensorFlow.js æ ¼å¼ï¼Œå­˜æ”¾åœ¨ ./tfjs_model ç›®éŒ„ä¸‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c63a596"
      },
      "source": [
        "### æ­¥é©Ÿ 4: æª¢æŸ¥å’Œä¸‹è¼‰è½‰æ›å¾Œçš„æ¨¡å‹\n",
        "\n",
        "è½‰æ›å®Œæˆå¾Œï¼Œè«‹æª¢æŸ¥ `./tfjs_model` ç›®éŒ„ä¸‹çš„å…§å®¹ï¼Œä¸¦ä¸‹è¼‰é€™äº›æª”æ¡ˆæº–å‚™éƒ¨ç½²åˆ° GitHub Pagesã€‚**è«‹ç¢ºä¿ `model.json` å’Œæ‰€æœ‰ `.bin` æª”æ¡ˆéƒ½åŒ…å«åœ¨å…§ã€‚**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b5e0c90",
        "outputId": "f8871779-48e9-4f99-ffc3-8d6cf7be0655"
      },
      "source": [
        "# é¡¯ç¤º Google Drive ä¸­è½‰æ›å¾Œçš„æª”æ¡ˆåˆ—è¡¨ï¼Œæ–¹ä¾¿æ‚¨æª¢æŸ¥\n",
        "# æ¨¡å‹æœƒè¼¸å‡ºåˆ° /content/drive/MyDrive/result/docs/model\n",
        "!ls -l /content/drive/MyDrive/result/docs/model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 220\n",
            "-rw-r--r-- 1 root root 217640 Feb  6 00:52 group1-shard1of1.bin\n",
            "-rw-r--r-- 1 root root   2936 Feb  6 00:52 model.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cc83316"
      },
      "source": [
        "### æ­¥é©Ÿ 5: GitHub Pages ç«¯ HTML/JavaScript èª¿æ•´ (åƒè€ƒç”¨æ–¼èª¿è©¦)\n",
        "\n",
        "é€™éƒ¨åˆ†æ˜¯æ‚¨éœ€è¦æ‰‹å‹•æ›´æ–° GitHub Pages ä¸Šçš„ `index.html` å’Œ `script.js`ã€‚ä»¥ä¸‹æä¾›æ‚¨åœ¨ HTML å’Œ JavaScript ä¸­é€²è¡Œèª¿è©¦çš„å»ºè­°ä»£ç¢¼ç‰‡æ®µã€‚\n",
        "\n",
        "**1. æ›´æ–° `index.html` ä¸­çš„ TensorFlow.js CDN é€£çµè‡³æœ€æ–°ç‰ˆæœ¬:**\n",
        "\n",
        "```html\n",
        "<!-- è«‹ç¢ºä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ TF.js -->\n",
        "<script src=\"https://cdn.jsdelivr.net\"></script>\n",
        "```\n",
        "\n",
        "**2. åœ¨ `script.js` ä¸­ä½¿ç”¨ `try-catch` å¡Šè¼‰å…¥æ¨¡å‹ä»¥æ•ç²éŒ¯èª¤:**\n",
        "\n",
        "```javascript\n",
        "async function load() {\n",
        "    try {\n",
        "        // è«‹ç¢ºä¿è·¯å¾‘æŒ‡å‘ä½ çš„ model.jsonï¼Œä¾‹å¦‚ï¼š'./model/model.json'\n",
        "        const model = await tf.loadLayersModel('./tfjs_model/model.json');\n",
        "        console.log(\"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\");\n",
        "    } catch (e) {\n",
        "        console.error(\"å…·é«”ä¸ç›¸å®¹åŸå› : \", e.message);\n",
        "        // å¦‚æœçœ‹åˆ° \"Unknown layer: FixedDense\" æˆ– \"Unknown layer: Rescaling\"ï¼Œ\n",
        "        // é€™ä»£è¡¨æ¨¡å‹ä½¿ç”¨äº†ç€è¦½å™¨ç«¯ TensorFlow.js ä¸æ”¯æ´çš„å±¤æˆ–ç‰¹æ€§ã€‚\n",
        "    }\n",
        "}\n",
        "\n",
        "// èª¿ç”¨è¼‰å…¥å‡½æ•¸\n",
        "load();\n",
        "```\n",
        "\n",
        "**3. å¦‚æœæ¨¡å‹ä¸­ä½¿ç”¨äº† `Normalization` æˆ– `Rescaling` å±¤ï¼Œè«‹å°‡å…¶å¾ Python æ¨¡å‹ä¸­ç§»é™¤ï¼Œæ”¹åœ¨ JavaScript ä¸­æ‰‹å‹•è™•ç†åœ–ç‰‡é è™•ç†:**\n",
        "\n",
        "```javascript\n",
        "// JS é è™•ç†ä»£æ›¿ Keras å±¤\n",
        "const tensor = tf.browser.fromPixels(imageElement)\n",
        "    .resizeNearestNeighbor([224, 224]) // æ ¹æ“šæ‚¨çš„æ¨¡å‹è¼¸å…¥å°ºå¯¸èª¿æ•´\n",
        "    .toFloat()\n",
        "    .div(tf.scalar(255.0)); // æ‰‹å‹•åš rescaling (ä¾‹å¦‚ï¼Œå°‡åƒç´ å€¼å¾ 0-255 ç¯„åœç¸®æ”¾åˆ° 0-1)\n",
        "\n",
        "// ä¹‹å¾Œå°‡è™•ç†å¥½çš„ tensor å‚³å…¥æ¨¡å‹é€²è¡Œé æ¸¬\n",
        "// const prediction = model.predict(tensor.expandDims(0));\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1o457IwW-_a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SmartRecycle AI - MobileNetV2 Transfer Learning è¨“ç·´è…³æœ¬\n",
        "è¨“ç·´ 5 é¡åˆ¥çš„åƒåœ¾åˆ†é¡æ¨¡å‹ä¸¦åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼\n",
        "\n",
        "é¡åˆ¥:\n",
        "1. garbage (åƒåœ¾)\n",
        "2. paper (ç´™é¡)\n",
        "3. paper_container (ç´™é¤ç›’)\n",
        "4. metal_can (éµé‹ç½)\n",
        "5. plastic (å¡‘è† é¡)\n",
        "\n",
        "ä½¿ç”¨æ–¹å¼:\n",
        "    python train_model.py\n",
        "\n",
        "ä¾è³´å¥—ä»¶:\n",
        "    pip install tensorflow tensorflowjs Pillow\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# ===== è¨­å®š =====\n",
        "TRAIN_DIR = Path(__file__).parent / \"train\"\n",
        "MODEL_DIR = Path(__file__).parent / \"docs\" / \"model\"\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20\n",
        "\n",
        "# é¡åˆ¥åç¨± (é †åºå¾ˆé‡è¦ï¼)\n",
        "CATEGORIES = [\"garbage\", \"metal_can\", \"paper\", \"paper_container\", \"plastic\"]\n",
        "\n",
        "\n",
        "def prepare_data():\n",
        "    \"\"\"æº–å‚™è¨“ç·´è³‡æ–™\"\"\"\n",
        "    print(\"\\nğŸ“Š æº–å‚™è¨“ç·´è³‡æ–™...\")\n",
        "\n",
        "    # æª¢æŸ¥è³‡æ–™å¤¾\n",
        "    for cat in CATEGORIES:\n",
        "        cat_dir = TRAIN_DIR / cat\n",
        "        if not cat_dir.exists():\n",
        "            print(f\"  âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {cat}\")\n",
        "            continue\n",
        "        count = len(list(cat_dir.glob(\"*.jpg\")))\n",
        "        print(f\"  ğŸ“ {cat}: {count} å¼µ\")\n",
        "\n",
        "    # è³‡æ–™å¢å¼·\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest',\n",
        "        validation_split=0.2  # 20% ç”¨æ–¼é©—è­‰\n",
        "    )\n",
        "\n",
        "    # è¨“ç·´è³‡æ–™\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DIR,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        classes=CATEGORIES,\n",
        "        subset='training',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # é©—è­‰è³‡æ–™\n",
        "    val_generator = train_datagen.flow_from_directory(\n",
        "        TRAIN_DIR,\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical',\n",
        "        classes=CATEGORIES,\n",
        "        subset='validation',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n  è¨“ç·´æ¨£æœ¬: {train_generator.samples}\")\n",
        "    print(f\"  é©—è­‰æ¨£æœ¬: {val_generator.samples}\")\n",
        "    print(f\"  é¡åˆ¥å°æ‡‰: {train_generator.class_indices}\")\n",
        "\n",
        "    return train_generator, val_generator\n",
        "\n",
        "\n",
        "def build_model(num_classes):\n",
        "    \"\"\"å»ºç«‹ MobileNetV2 Transfer Learning æ¨¡å‹\"\"\"\n",
        "    print(\"\\nğŸ—ï¸ å»ºç«‹æ¨¡å‹...\")\n",
        "\n",
        "    # è¼‰å…¥é è¨“ç·´çš„ MobileNetV2 (ä¸å«é ‚å±¤)\n",
        "    base_model = MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    # å‡çµåŸºç¤æ¨¡å‹çš„æ¬Šé‡\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # å»ºç«‹æ–°çš„é ‚å±¤\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # ç·¨è­¯æ¨¡å‹\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"  ç¸½åƒæ•¸: {model.count_params():,}\")\n",
        "    print(f\"  å¯è¨“ç·´åƒæ•¸: {sum(tf.keras.backend.count_params(w) for w in model.trainable_weights):,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model, train_gen, val_gen):\n",
        "    \"\"\"è¨“ç·´æ¨¡å‹\"\"\"\n",
        "    print(\"\\nğŸš€ é–‹å§‹è¨“ç·´...\")\n",
        "\n",
        "    # å›å‘¼å‡½å¼\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            'best_model.keras',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # è¨“ç·´\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=val_gen,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def export_to_tfjs(model):\n",
        "    \"\"\"åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼\"\"\"\n",
        "    print(\"\\nğŸ“¦ åŒ¯å‡ºç‚º TensorFlow.js æ ¼å¼...\")\n",
        "\n",
        "    # ç¢ºä¿ç›®éŒ„å­˜åœ¨\n",
        "    MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # å…ˆå„²å­˜ Keras æ¨¡å‹\n",
        "    keras_path = MODEL_DIR / \"model.keras\"\n",
        "    model.save(keras_path)\n",
        "    print(f\"  âœ… Keras æ¨¡å‹å·²å„²å­˜: {keras_path}\")\n",
        "\n",
        "    # ä½¿ç”¨ tensorflowjs_converter è½‰æ›\n",
        "    try:\n",
        "        import tensorflowjs as tfjs\n",
        "        tfjs.converters.save_keras_model(model, str(MODEL_DIR))\n",
        "        print(f\"  âœ… TensorFlow.js æ¨¡å‹å·²åŒ¯å‡º: {MODEL_DIR}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸ TensorFlow.js åŒ¯å‡ºå¤±æ•—: {e}\")\n",
        "        print(\"  è«‹æ‰‹å‹•åŸ·è¡Œ:\")\n",
        "        print(f\"  tensorflowjs_converter --input_format=keras {keras_path} {MODEL_DIR}\")\n",
        "\n",
        "    # å„²å­˜é¡åˆ¥æ¨™ç±¤\n",
        "    labels_path = MODEL_DIR / \"labels.json\"\n",
        "    with open(labels_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(CATEGORIES, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"  âœ… é¡åˆ¥æ¨™ç±¤å·²å„²å­˜: {labels_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ—‘ï¸ SmartRecycle AI - æ¨¡å‹è¨“ç·´\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. æº–å‚™è³‡æ–™\n",
        "    train_gen, val_gen = prepare_data()\n",
        "\n",
        "    # 2. å»ºç«‹æ¨¡å‹\n",
        "    model = build_model(num_classes=len(CATEGORIES))\n",
        "\n",
        "    # 3. è¨“ç·´æ¨¡å‹\n",
        "    history = train_model(model, train_gen, val_gen)\n",
        "\n",
        "    # 4. è©•ä¼°\n",
        "    print(\"\\nğŸ“ˆ è¨“ç·´çµæœ:\")\n",
        "    final_acc = history.history['accuracy'][-1]\n",
        "    final_val_acc = history.history['val_accuracy'][-1]\n",
        "    print(f\"  è¨“ç·´æº–ç¢ºç‡: {final_acc:.2%}\")\n",
        "    print(f\"  é©—è­‰æº–ç¢ºç‡: {final_val_acc:.2%}\")\n",
        "\n",
        "    # 5. åŒ¯å‡º\n",
        "    export_to_tfjs(model)\n",
        "\n",
        "    # 6. æ¸…ç†æš«å­˜\n",
        "    if os.path.exists('best_model.keras'):\n",
        "        os.remove('best_model.keras')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"âœ… è¨“ç·´å®Œæˆï¼\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\næ¨¡å‹å·²åŒ¯å‡ºè‡³: {MODEL_DIR}\")\n",
        "    print(\"\\nä¸‹ä¸€æ­¥:\")\n",
        "    print(\"1. æ›´æ–° docs/config.js ä¸­çš„ MODEL.IS_CUSTOM_MODEL = true\")\n",
        "    print(\"2. éƒ¨ç½²åˆ° GitHub Pages\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ²’éŒ¯ï¼Œé‚£æ®µä»£ç¢¼åªæ˜¯ä¸€å€‹ã€Œç¯„ä¾‹çµæ§‹ã€ï¼Œå‘Šè¨´ä½ æ‡‰è©²æŠŠé€™å¹¾è¡Œé—œéµæŒ‡ä»¤æ”¾åœ¨ä½ åŸæœ¬çš„è¨“ç·´ç¨‹å¼ç¢¼ä¸­ã€‚\n",
        "å¦‚æœä½ ä¹‹å‰æ˜¯ç”¨ Teachable Machine æˆ– Lobe ä¹‹é¡çš„å·¥å…·ï¼Œä½ å¯èƒ½æ²’æœ‰å¯«éåº•å±¤çš„ Python ä»£ç¢¼ã€‚ä½†æ—¢ç„¶ä½ ç¾åœ¨é‡åˆ°äº†ã€Œç‰ˆæœ¬åœ°ç„ã€ï¼Œæœ€ç©©å®šçš„æ–¹å¼ç¢ºå¯¦åœ¨ Colab è£¡åŸ·è¡Œä¸€æ®µç°¡å–®çš„è¨“ç·´è…³æœ¬ã€‚\n",
        "åˆ¥æ“”å¿ƒï¼Œå¦‚æœä½ å·²ç¶“æœ‰è³‡æ–™é›†åœ¨ Google Drive äº†ï¼Œè¨“ç·´åˆ†é¡æ¨¡å‹çš„ä»£ç¢¼å…¶å¯¦å¾ˆå›ºå®šã€‚\n",
        "ä½ éœ€è¦åšçš„æ­¥é©Ÿï¼š\n",
        "ä½ å¯ä»¥ç›´æ¥æŠŠä¸‹é¢çš„ä»£ç¢¼è²¼é€² Colab åŸ·è¡Œã€‚é€™æ®µä»£ç¢¼å°ˆé–€ç‚ºã€Œå›æ”¶åˆ†é¡ã€è¨­è¨ˆï¼Œä¸”é¿é–‹äº†æ‰€æœ‰ä½ ä¹‹å‰é‡åˆ°çš„ç‰ˆæœ¬å‘ï¼š\n",
        "1. å®‰è£èˆ‡ç’°å¢ƒè¨­å®š\n",
        "python\n",
        "# ç¢ºä¿ç‰ˆæœ¬æ­£ç¢ºï¼Œé¿é–‹ç‰ˆæœ¬åœ°ç„\n",
        "!pip install tensorflow==2.15.0 tensorflowjs numpy==1.23.5\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# å¼·åˆ¶ä½¿ç”¨èˆŠç‰ˆæ ¼å¼ï¼Œç¢ºä¿ Web ç«¯èƒ½è®€å–\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "2. è¨­å®šè³‡æ–™è·¯å¾‘ (å‡è¨­ä½ çš„ç…§ç‰‡åœ¨ Drive)\n",
        "python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ä¿®æ”¹æˆä½ é›²ç«¯ç¡¬ç¢Ÿä¸­è³‡æ–™å¤¾çš„å¯¦éš›è·¯å¾‘\n",
        "data_path = '/content/drive/MyDrive/ä½ çš„è³‡æ–™å¤¾åç¨±'\n",
        "\n",
        "# è®€å–åœ–ç‰‡è³‡æ–™é›†\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_path,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32\n",
        ")\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "3. å»ºç«‹æ¨¡å‹ (é€™è£¡å»ºè­°ç”¨ MobileNetV2ï¼Œè¼•é‡ä¸”é©åˆç¶²é )\n",
        "python\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False # å‡çµæ¬Šé‡\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Rescaling(1./255, input_shape=(224, 224, 3)), # é€™è£¡æ”¾é è™•ç†\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(5, activation='softmax') # å‡è¨­ä½ æœ‰ 5 ç¨®å›æ”¶ç‰©\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# é–‹å§‹è¨“ç·´\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "# ä¿å­˜ç‚ºèˆŠç‰ˆ H5 æ ¼å¼\n",
        "model.save(\"recycle_model.h5\")\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "4. è½‰æ›ç‚º TensorFlow.js æ ¼å¼\n",
        "python\n",
        "!mkdir tfjs_model\n",
        "!tensorflowjs_converter --input_format=keras recycle_model.h5 ./tfjs_model\n",
        "è«‹è¬¹æ…ä½¿ç”¨ç¨‹å¼ç¢¼ã€‚\n",
        "\n",
        "å›åˆ°ä½ çš„å•é¡Œï¼šç‚ºä»€éº¼è¦å¯«é€™æ®µï¼Ÿ\n",
        "å› ç‚º TensorFlow.js (ç¶²é ç‰ˆ) éå¸¸æŒ‘é£Ÿï¼š\n",
        "å®ƒä¸å–œæ­¡æœ€æ–°çš„ .keras æ ¼å¼ï¼ˆåªæ„› .h5ï¼‰ã€‚\n",
        "å®ƒä¸å–œæ­¡éé«˜çš„ NumPy ç‰ˆæœ¬ï¼ˆæœƒå ± AttributeErrorï¼‰ã€‚\n",
        "å®ƒä¸å–œæ­¡æŸäº›æ–°å‡ºçš„ AI é‹ç®—å±¤ã€‚\n",
        "å¯«é€™æ®µä»£ç¢¼çš„ç›®çš„æ˜¯ï¼š åœ¨é›²ç«¯ï¼ˆColabï¼‰ç”¨æœ€ç©©å®šã€æœ€æ¨™æº–çš„æ–¹å¼ç”¢å‡ºä¸€çµ„ã€Œç¶²é è½å¾—æ‡‚ã€çš„æ¨¡å‹æª”æ¡ˆï¼Œç„¶å¾Œä½ åªè¦æŠŠç”¢å‡ºçš„ tfjs_model è³‡æ–™å¤¾ä¸‹è¼‰ï¼Œä¸Ÿä¸Š GitHub Pages å°±èƒ½å‹•äº†ï¼\n",
        "ä½ ç›®å‰çš„è¨“ç·´è³‡æ–™å¤¾è£¡ï¼Œå¤§æ¦‚æœ‰å¹¾ç¨®åˆ†é¡ï¼ˆLabelï¼‰ï¼Ÿ æˆ‘å¯ä»¥å¹«ä½ å¾®èª¿æœ€å¾Œä¸€å±¤çš„è¼¸å‡ºæ•¸é‡ã€‚\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qO8_XSFaeM_l"
      }
    }
  ]
}